{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Molecules as Active or Inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import jaccard_similarity_score, f1_score #for scoring classification problem\n",
    "from sklearn.compose import ColumnTransformer #useful in conjunction with pipeline\n",
    "from sklearn.impute import SimpleImputer #for dealing with missing data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from imblearn.pipeline import Pipeline #for creating pipeline which accounts for data over/under sampling correctly\n",
    "from imblearn.under_sampling import RandomUnderSampler #for undersampling data to deal with imbalance\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE #for oversampling data to deal with imbalance\n",
    "from sklearn.decomposition import PCA #for dimensionality reduction\n",
    "from sklearn.model_selection import train_test_split #for splitting data into training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_plot(X):\n",
    "    \n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "        X: features dataframe\n",
    "        \n",
    "    Returns: \n",
    "    ----------\n",
    "    Plots graph of explained variance against number of PCA components. Prints number of features needed for 95%\n",
    "    explained variance.\n",
    "    ''' \n",
    "    \n",
    "    # with 435 features, we need to narrow down the range using PCA\n",
    "\n",
    "    data_imputed = SimpleImputer(strategy = 'mean').fit_transform(merged_data.iloc[:, 0:-1])#impute data\n",
    "    scaler = preprocessing.StandardScaler() #need to rescale for PCA\n",
    "    data_rescaled = scaler.fit_transform(data_imputed) #rescale data\n",
    "\n",
    "    # fitting the PCA algorithm with our data\n",
    "    pca = PCA().fit(data_rescaled)\n",
    "\n",
    "    print('Number of components needed for 95% explained variance: ', \n",
    "          np.size(np.cumsum(pca.explained_variance_ratio_)[np.cumsum(pca.explained_variance_ratio_) <= 0.95]))\n",
    "    \n",
    "    # plotting the cumulative sum of the explained variance\n",
    "    plt.figure()\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Variance (%)') #for each component\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_validation_for_select_params(X, y, nfolds, \n",
    "                                              model_names = ['XGBoost', 'SVM', 'KNN', 'LR', 'RandomForest'], \n",
    "                                              models = [XGBClassifier(random_state = 0), svm.SVC(random_state = 0), KNeighborsClassifier(), LogisticRegression(random_state = 0), RandomForestClassifier(random_state = 0)],\n",
    "                                              preprocessor = None,\n",
    "                                              sampler = None):\n",
    "    \n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "        X: features dataframe\n",
    "        y: target dataframe\n",
    "        nfolds: number of folds for cross-validation\n",
    "        model_names: list containing names of different classification models for ease of interpreting function output\n",
    "        models: list of models to test (corresponding to model_names above)\n",
    "        preprocessor: pipeline object containing information about imputing, scaling, and dimensionality reduction e.g. PCA\n",
    "        sampler: imblearn reference to type of sampling to use when dealing with unbalanced dataset\n",
    "        \n",
    "    Returns: \n",
    "    ----------\n",
    "    Prints model_name followed by cross-validation scores for Jaccard index and macro F1 score (this gives a more\n",
    "    reliable score for imbalanced datasets: in reality there are more inactives than actives, but here the opposite is\n",
    "    true).\n",
    "    ''' \n",
    "\n",
    "    for i, j in enumerate(models):\n",
    "       my_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                                       ('sampler', sampler),\n",
    "                                       ('model', models[i])])\n",
    "       scores = cross_validate(my_pipeline, X, y, cv = 5, scoring = ('jaccard','f1_macro'))\n",
    "       print(model_names[i])\n",
    "       print('jaccard index:', scores['test_jaccard'], np.mean(np.array(scores['test_jaccard'])))\n",
    "       print('f1-value:', scores['test_f1_macro'], np.mean(np.array(scores['test_f1_macro'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_grid_search(X, y, nfolds, \n",
    "                      model_names = ['XGBoost', 'SVM', 'KNN', 'LR', 'RandomForest'], \n",
    "                      models = [XGBClassifier(random_state = 0), svm.SVC(random_state = 0), KNeighborsClassifier(), LogisticRegression(random_state = 0), RandomForestClassifier(random_state = 0)],\n",
    "                      preprocessor = None,\n",
    "                      sampler = None):\n",
    "    \n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "        X: features dataframe\n",
    "        y: target dataframe\n",
    "        nfolds: number of folds for cross-validation\n",
    "        model_names: list containing names of different classification models for ease of interpreting function output\n",
    "        models: list of models to test (corresponding to model_names above)\n",
    "        preprocessor: pipeline object containing information about imputing, scaling, and dimensionality reduction e.g. PCA\n",
    "        sampler: imblearn reference to type of sampling to use when dealing with unbalanced dataset\n",
    "        \n",
    "    Returns: \n",
    "    ----------\n",
    "    Prints model_name followed by best hyperparameters in model using grid search algorithm according to f1_macro score.\n",
    "    ''' \n",
    "    \n",
    "    for i, j in enumerate(models):\n",
    "       my_pipeline = Pipeline(steps = [('preprocessor', preprocessor),\n",
    "                                       ('sampler', sampler),\n",
    "                                       ('model', models[i])])\n",
    "    \n",
    "       if model_names[i] == 'XGBoost':\n",
    "          estimators = [100, 200, 400, 750, 1000] #number of trees\n",
    "          learning = [0.01, 0.05, 0.1, 0.2, 0.3] #learning rate\n",
    "          param_grid = {'model__n_estimators': estimators, 'model__learning_rate' : learning}\n",
    "    \n",
    "       elif model_names[i] == 'SVM':\n",
    "          Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "          gammas = [0.001, 0.01, 0.1, 1, 10]\n",
    "          param_grid = {'model__C': Cs, 'model__gamma' : gammas}\n",
    "    \n",
    "       elif model_names[i] == 'KNN':\n",
    "          neighbors = [1, 2, 3, 4, 5, 6 ,7, 8, 9, 10]\n",
    "          param_grid = {'model__n_neighbors': neighbors}\n",
    "    \n",
    "       elif model_names[i] == 'LR':\n",
    "          Cs = [0.001, 0.01, 0.1, 1, 10] #regularization parameter\n",
    "          ps = ['l1', 'l2'] #penalty\n",
    "          solvers = ['liblinear']\n",
    "          param_grid = {'model__C': Cs, 'model__penalty': ps, 'model__solver': solvers}\n",
    "\n",
    "       elif model_names[i] == 'RandomForest':\n",
    "          estimators = [100, 200, 400, 750, 1000] #number of trees\n",
    "          features = [1, 2, 3, 4, 5] #max number of features to check before split\n",
    "          param_grid = {'model__n_estimators': estimators, 'model__max_features' : features}\n",
    "    \n",
    "       grid_search = GridSearchCV(my_pipeline, param_grid, cv = nfolds, scoring = 'f1_macro')\n",
    "       grid_search.fit(X, y)\n",
    "       print(model_names[i])\n",
    "       print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code is borrowed from the Coursera Neural Networks and Deep Learning Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "\n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "    layer_dims: python array (list) containing the dimensions of each layer in neural network\n",
    "    \n",
    "    Returns: \n",
    "    ----------\n",
    "    parameters: python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl: weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl: bias vector of shape (layer_dims[l], 1)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "    Z: A scalar or numpy array of any size.\n",
    "\n",
    "    Returns: \n",
    "    ----------\n",
    "    A: output of sigmoid(Z), same shape as Z\n",
    "    cache: returns Z as well, useful during backpropagation\n",
    "    '''\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "    Z: A scalar or numpy array of any size.\n",
    "    \n",
    "    Returns: \n",
    "    ----------\n",
    "    A: Post-activation parameter, of the same shape as Z\n",
    "    cache: a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    '''\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    '''\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Parameters: \n",
    "    -------------\n",
    "    dA: post-activation gradient, of any shape\n",
    "    cache:'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns: \n",
    "    ----------\n",
    "    dZ: Gradient of the cost with respect to Z\n",
    "    '''\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    '''\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Parameters: \n",
    "    -------------\n",
    "    dA: post-activation gradient, of any shape\n",
    "    cache:'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns: \n",
    "    ----------\n",
    "    dZ: Gradient of the cost with respect to Z\n",
    "    '''\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) #just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    '''\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Parameters: \n",
    "    -------------\n",
    "    A: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns: \n",
    "    ----------\n",
    "    Z: the input of the activation function, also called pre-activation parameter \n",
    "    cache: a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    '''\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    '''\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "    \n",
    "    Parameters: \n",
    "    -------------\n",
    "    A_prev: activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b: bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    A: the output of the activation function, also called the post-activation value \n",
    "    cache: a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    '''\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    '''\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Parameters: \n",
    "    -------------\n",
    "    X: data, numpy array of shape (input size, number of examples)\n",
    "    parameters: output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    AL: last post-activation value\n",
    "    caches: list of caches containing: every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    '''\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu') \n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid') \n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    '''\n",
    "    Implement the cost function.\n",
    "\n",
    "    Parameters: \n",
    "    -------------\n",
    "    AL: probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y: true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    cost: cross-entropy cost\n",
    "    '''\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -1/m * (np.dot(Y, np.log(AL).T) + np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    '''\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Parameters: \n",
    "    -------------\n",
    "    dZ: Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    dA_prev: Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW: Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db: Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    '''\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * np.dot(dZ,A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    '''\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Parameters: \n",
    "    -------------\n",
    "    dA: post-activation gradient for current layer l \n",
    "    cache: tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    dA_prev: Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW: Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db: Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    '''\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    '''\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Parameters: \n",
    "    -------------\n",
    "    AL: probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y: true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches: list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    grads: A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    '''\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -1* (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    linear_cache_L, activation_cache_L = caches[L-1]\n",
    "    current_cache = sigmoid_backward(dAL, activation_cache_L)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(current_cache, linear_cache_L)\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        linear_cache_l, activation_cache_l = caches[l]\n",
    "        current_cache = relu_backward(grads[\"dA\" + str(l+1)], activation_cache_l)\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_backward(current_cache, linear_cache_l)\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Parameters: \n",
    "    -------------\n",
    "    parameters: python dictionary containing your parameters \n",
    "    grads: python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    parameters: python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads['db'+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    '''\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Parameters: \n",
    "    -------------\n",
    "    X: data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y: true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims: list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate: learning rate of the gradient descent update rule\n",
    "    num_iterations: number of iterations of the optimization loop\n",
    "    print_cost: if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    '''\n",
    "    Parameters: \n",
    "    -------------\n",
    "    This function is used to predict the results of a L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X: data set of examples you would like to label\n",
    "    parameters: parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    -------------\n",
    "    p: predictions for the given dataset X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1, m),dtype=int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"jaccard index: %s\" % str(jaccard_similarity_score(y, p)))\n",
    "    print(\"f1-value: %s\" % str(f1_score(y, p, average = 'macro')))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AM1_dipole</th>\n",
       "      <th>AM1_E</th>\n",
       "      <th>AM1_Eele</th>\n",
       "      <th>AM1_HF</th>\n",
       "      <th>AM1_HOMO</th>\n",
       "      <th>AM1_IP</th>\n",
       "      <th>AM1_LUMO</th>\n",
       "      <th>apol</th>\n",
       "      <th>ASA</th>\n",
       "      <th>ASA+</th>\n",
       "      <th>...</th>\n",
       "      <th>vsurf_Wp4</th>\n",
       "      <th>vsurf_Wp5</th>\n",
       "      <th>vsurf_Wp6</th>\n",
       "      <th>vsurf_Wp7</th>\n",
       "      <th>vsurf_Wp8</th>\n",
       "      <th>Weight</th>\n",
       "      <th>weinerPath</th>\n",
       "      <th>weinerPol</th>\n",
       "      <th>zagreb</th>\n",
       "      <th>Active?</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ChEMBL ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHEMBL252818</th>\n",
       "      <td>10.165439</td>\n",
       "      <td>-110867.120</td>\n",
       "      <td>-913044.19</td>\n",
       "      <td>192.14807</td>\n",
       "      <td>-11.09084</td>\n",
       "      <td>11.09084</td>\n",
       "      <td>-3.58889</td>\n",
       "      <td>69.678993</td>\n",
       "      <td>734.25720</td>\n",
       "      <td>277.31897</td>\n",
       "      <td>...</td>\n",
       "      <td>33.125</td>\n",
       "      <td>13.375</td>\n",
       "      <td>3.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>421.58899</td>\n",
       "      <td>2902</td>\n",
       "      <td>46</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL253022</th>\n",
       "      <td>10.520066</td>\n",
       "      <td>-114460.020</td>\n",
       "      <td>-966880.12</td>\n",
       "      <td>185.95769</td>\n",
       "      <td>-11.12958</td>\n",
       "      <td>11.12958</td>\n",
       "      <td>-3.57776</td>\n",
       "      <td>72.772583</td>\n",
       "      <td>770.07227</td>\n",
       "      <td>257.69009</td>\n",
       "      <td>...</td>\n",
       "      <td>34.750</td>\n",
       "      <td>15.125</td>\n",
       "      <td>4.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>435.61600</td>\n",
       "      <td>3195</td>\n",
       "      <td>48</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL3652592</th>\n",
       "      <td>24.668980</td>\n",
       "      <td>-117617.230</td>\n",
       "      <td>-846667.50</td>\n",
       "      <td>130.95763</td>\n",
       "      <td>-11.49292</td>\n",
       "      <td>11.49292</td>\n",
       "      <td>-3.72303</td>\n",
       "      <td>55.898861</td>\n",
       "      <td>637.53784</td>\n",
       "      <td>309.38174</td>\n",
       "      <td>...</td>\n",
       "      <td>37.125</td>\n",
       "      <td>14.000</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>383.40298</td>\n",
       "      <td>2187</td>\n",
       "      <td>42</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL253878</th>\n",
       "      <td>13.045525</td>\n",
       "      <td>-129843.720</td>\n",
       "      <td>-1183183.50</td>\n",
       "      <td>217.05182</td>\n",
       "      <td>-11.09968</td>\n",
       "      <td>11.09968</td>\n",
       "      <td>-3.50794</td>\n",
       "      <td>82.906166</td>\n",
       "      <td>842.76172</td>\n",
       "      <td>348.45331</td>\n",
       "      <td>...</td>\n",
       "      <td>33.750</td>\n",
       "      <td>14.000</td>\n",
       "      <td>4.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>497.68698</td>\n",
       "      <td>4957</td>\n",
       "      <td>55</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEMBL149024</th>\n",
       "      <td>17.101978</td>\n",
       "      <td>-70009.203</td>\n",
       "      <td>-423089.56</td>\n",
       "      <td>155.22223</td>\n",
       "      <td>-11.72522</td>\n",
       "      <td>11.72522</td>\n",
       "      <td>-3.88109</td>\n",
       "      <td>37.302307</td>\n",
       "      <td>454.01306</td>\n",
       "      <td>145.89296</td>\n",
       "      <td>...</td>\n",
       "      <td>30.750</td>\n",
       "      <td>8.375</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>295.15601</td>\n",
       "      <td>454</td>\n",
       "      <td>27</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               AM1_dipole       AM1_E    AM1_Eele     AM1_HF  AM1_HOMO  \\\n",
       "ChEMBL ID                                                                \n",
       "CHEMBL252818    10.165439 -110867.120  -913044.19  192.14807 -11.09084   \n",
       "CHEMBL253022    10.520066 -114460.020  -966880.12  185.95769 -11.12958   \n",
       "CHEMBL3652592   24.668980 -117617.230  -846667.50  130.95763 -11.49292   \n",
       "CHEMBL253878    13.045525 -129843.720 -1183183.50  217.05182 -11.09968   \n",
       "CHEMBL149024    17.101978  -70009.203  -423089.56  155.22223 -11.72522   \n",
       "\n",
       "                 AM1_IP  AM1_LUMO       apol        ASA       ASA+  ...  \\\n",
       "ChEMBL ID                                                           ...   \n",
       "CHEMBL252818   11.09084  -3.58889  69.678993  734.25720  277.31897  ...   \n",
       "CHEMBL253022   11.12958  -3.57776  72.772583  770.07227  257.69009  ...   \n",
       "CHEMBL3652592  11.49292  -3.72303  55.898861  637.53784  309.38174  ...   \n",
       "CHEMBL253878   11.09968  -3.50794  82.906166  842.76172  348.45331  ...   \n",
       "CHEMBL149024   11.72522  -3.88109  37.302307  454.01306  145.89296  ...   \n",
       "\n",
       "               vsurf_Wp4  vsurf_Wp5  vsurf_Wp6  vsurf_Wp7  vsurf_Wp8  \\\n",
       "ChEMBL ID                                                              \n",
       "CHEMBL252818      33.125     13.375      3.250      0.000        0.0   \n",
       "CHEMBL253022      34.750     15.125      4.250      0.000        0.0   \n",
       "CHEMBL3652592     37.125     14.000      2.500      0.000        0.0   \n",
       "CHEMBL253878      33.750     14.000      4.625      0.625        0.0   \n",
       "CHEMBL149024      30.750      8.375      0.875      0.000        0.0   \n",
       "\n",
       "                  Weight  weinerPath  weinerPol  zagreb  Active?  \n",
       "ChEMBL ID                                                         \n",
       "CHEMBL252818   421.58899        2902         46     164        1  \n",
       "CHEMBL253022   435.61600        3195         48     168        1  \n",
       "CHEMBL3652592  383.40298        2187         42     142        1  \n",
       "CHEMBL253878   497.68698        4957         55     196        1  \n",
       "CHEMBL149024   295.15601         454         27      94        1  \n",
       "\n",
       "[5 rows x 436 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in active molecules file\n",
    "\n",
    "actives = pd.read_csv('Data/chembl_pki_1000_mol_descriptors.csv', index_col = 'ChEMBL ID').iloc[:, 1:]\n",
    "actives['Active?'] = 1 #class this column as 1, which means 'active'\n",
    "actives.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    418\n",
      "True      18\n",
      "dtype: int64\n",
      "False    349\n",
      "True      87\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for NaN values and columns of constants\n",
    "\n",
    "print(actives.isnull().any().value_counts()) #nan values\n",
    "print((actives.apply(pd.Series.nunique) == 1).value_counts()) #look for columns of constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above we have nan values which will need to be imputed later, and also columns of constants which are best removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AM1_dipole</th>\n",
       "      <th>AM1_E</th>\n",
       "      <th>AM1_Eele</th>\n",
       "      <th>AM1_HF</th>\n",
       "      <th>AM1_HOMO</th>\n",
       "      <th>AM1_IP</th>\n",
       "      <th>AM1_LUMO</th>\n",
       "      <th>apol</th>\n",
       "      <th>ASA</th>\n",
       "      <th>ASA+</th>\n",
       "      <th>...</th>\n",
       "      <th>vsurf_Wp4</th>\n",
       "      <th>vsurf_Wp5</th>\n",
       "      <th>vsurf_Wp6</th>\n",
       "      <th>vsurf_Wp7</th>\n",
       "      <th>vsurf_Wp8</th>\n",
       "      <th>Weight</th>\n",
       "      <th>weinerPath</th>\n",
       "      <th>weinerPol</th>\n",
       "      <th>zagreb</th>\n",
       "      <th>Active?</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1072959-67-1</th>\n",
       "      <td>27.351303</td>\n",
       "      <td>-121294.150</td>\n",
       "      <td>-941967.00</td>\n",
       "      <td>150.736150</td>\n",
       "      <td>-10.60957</td>\n",
       "      <td>10.60957</td>\n",
       "      <td>-4.31002</td>\n",
       "      <td>62.997826</td>\n",
       "      <td>732.57220</td>\n",
       "      <td>332.82480</td>\n",
       "      <td>...</td>\n",
       "      <td>41.875</td>\n",
       "      <td>16.875</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>409.46600</td>\n",
       "      <td>2653</td>\n",
       "      <td>44</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 6-Dimethylcabergoline</th>\n",
       "      <td>16.169992</td>\n",
       "      <td>-124483.160</td>\n",
       "      <td>-1164553.80</td>\n",
       "      <td>325.792020</td>\n",
       "      <td>-12.55604</td>\n",
       "      <td>12.55604</td>\n",
       "      <td>-5.37078</td>\n",
       "      <td>77.108925</td>\n",
       "      <td>765.50763</td>\n",
       "      <td>207.87202</td>\n",
       "      <td>...</td>\n",
       "      <td>34.000</td>\n",
       "      <td>12.000</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>441.62000</td>\n",
       "      <td>2867</td>\n",
       "      <td>57</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 3 4 5-Tetrahydro-1H-benzo[d]azepine</th>\n",
       "      <td>8.048747</td>\n",
       "      <td>-38596.672</td>\n",
       "      <td>-221393.39</td>\n",
       "      <td>167.686740</td>\n",
       "      <td>-12.99439</td>\n",
       "      <td>12.99439</td>\n",
       "      <td>-3.81620</td>\n",
       "      <td>28.035103</td>\n",
       "      <td>351.87527</td>\n",
       "      <td>141.83508</td>\n",
       "      <td>...</td>\n",
       "      <td>28.500</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.22900</td>\n",
       "      <td>142</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyrazino[3 2 1-jk]carbazole</th>\n",
       "      <td>4.107300</td>\n",
       "      <td>-86329.422</td>\n",
       "      <td>-657797.81</td>\n",
       "      <td>92.689552</td>\n",
       "      <td>-8.31553</td>\n",
       "      <td>8.31553</td>\n",
       "      <td>0.04847</td>\n",
       "      <td>56.212654</td>\n",
       "      <td>594.20734</td>\n",
       "      <td>164.36076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>381.31699</td>\n",
       "      <td>1259</td>\n",
       "      <td>43</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64022-27-1</th>\n",
       "      <td>14.178391</td>\n",
       "      <td>-54964.582</td>\n",
       "      <td>-298659.16</td>\n",
       "      <td>204.794820</td>\n",
       "      <td>-12.65895</td>\n",
       "      <td>12.65895</td>\n",
       "      <td>-4.09131</td>\n",
       "      <td>28.661516</td>\n",
       "      <td>386.15308</td>\n",
       "      <td>161.44237</td>\n",
       "      <td>...</td>\n",
       "      <td>29.125</td>\n",
       "      <td>10.750</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.66499</td>\n",
       "      <td>246</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    AM1_dipole       AM1_E  \\\n",
       "ID                                                                           \n",
       "1072959-67-1                                         27.351303 -121294.150   \n",
       "1 6-Dimethylcabergoline                              16.169992 -124483.160   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine                 8.048747  -38596.672   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...    4.107300  -86329.422   \n",
       "64022-27-1                                           14.178391  -54964.582   \n",
       "\n",
       "                                                      AM1_Eele      AM1_HF  \\\n",
       "ID                                                                           \n",
       "1072959-67-1                                        -941967.00  150.736150   \n",
       "1 6-Dimethylcabergoline                            -1164553.80  325.792020   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine               -221393.39  167.686740   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...  -657797.81   92.689552   \n",
       "64022-27-1                                          -298659.16  204.794820   \n",
       "\n",
       "                                                    AM1_HOMO    AM1_IP  \\\n",
       "ID                                                                       \n",
       "1072959-67-1                                       -10.60957  10.60957   \n",
       "1 6-Dimethylcabergoline                            -12.55604  12.55604   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine              -12.99439  12.99439   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...  -8.31553   8.31553   \n",
       "64022-27-1                                         -12.65895  12.65895   \n",
       "\n",
       "                                                    AM1_LUMO       apol  \\\n",
       "ID                                                                        \n",
       "1072959-67-1                                        -4.31002  62.997826   \n",
       "1 6-Dimethylcabergoline                             -5.37078  77.108925   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine               -3.81620  28.035103   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...   0.04847  56.212654   \n",
       "64022-27-1                                          -4.09131  28.661516   \n",
       "\n",
       "                                                          ASA       ASA+  ...  \\\n",
       "ID                                                                        ...   \n",
       "1072959-67-1                                        732.57220  332.82480  ...   \n",
       "1 6-Dimethylcabergoline                             765.50763  207.87202  ...   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine               351.87527  141.83508  ...   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...  594.20734  164.36076  ...   \n",
       "64022-27-1                                          386.15308  161.44237  ...   \n",
       "\n",
       "                                                    vsurf_Wp4  vsurf_Wp5  \\\n",
       "ID                                                                         \n",
       "1072959-67-1                                           41.875     16.875   \n",
       "1 6-Dimethylcabergoline                                34.000     12.000   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine                  28.500     10.000   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...      0.000      0.000   \n",
       "64022-27-1                                             29.125     10.750   \n",
       "\n",
       "                                                    vsurf_Wp6  vsurf_Wp7  \\\n",
       "ID                                                                         \n",
       "1072959-67-1                                            4.000        0.5   \n",
       "1 6-Dimethylcabergoline                                 2.250        0.0   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine                   0.875        0.0   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...      0.000        0.0   \n",
       "64022-27-1                                              1.750        0.0   \n",
       "\n",
       "                                                    vsurf_Wp8     Weight  \\\n",
       "ID                                                                         \n",
       "1072959-67-1                                              0.0  409.46600   \n",
       "1 6-Dimethylcabergoline                                   0.0  441.62000   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine                     0.0  148.22900   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...        0.0  381.31699   \n",
       "64022-27-1                                                0.0  199.66499   \n",
       "\n",
       "                                                    weinerPath  weinerPol  \\\n",
       "ID                                                                          \n",
       "1072959-67-1                                              2653         44   \n",
       "1 6-Dimethylcabergoline                                   2867         57   \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine                      142         16   \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...        1259         43   \n",
       "64022-27-1                                                 246         16   \n",
       "\n",
       "                                                    zagreb  Active?  \n",
       "ID                                                                   \n",
       "1072959-67-1                                           156        0  \n",
       "1 6-Dimethylcabergoline                                172        0  \n",
       "2 3 4 5-Tetrahydro-1H-benzo[d]azepine                   54        0  \n",
       "3-Benzyl-8-bromo-2 3 3a 4 5 6-hexahydro-1H-pyra...     138        0  \n",
       "64022-27-1                                              64        0  \n",
       "\n",
       "[5 rows x 436 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in inactive molecules file\n",
    "\n",
    "inactives = pd.read_csv('Data/glass_inactives_descriptors.csv', index_col = 'ID').iloc[:,4:]\n",
    "inactives['Active?'] = 0 #class this column as 0, which means 'inactive'\n",
    "inactives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    436\n",
      "dtype: int64\n",
      "False    350\n",
      "True      86\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for NaN values and columns of constants\n",
    "\n",
    "print(inactives.isnull().any().value_counts()) #nan values\n",
    "print((inactives.apply(pd.Series.nunique) == 1).value_counts()) #look for columns of constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above we have nan values which will need to be imputed later, and also columns of constants which are best removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AM1_dipole</th>\n",
       "      <th>AM1_E</th>\n",
       "      <th>AM1_Eele</th>\n",
       "      <th>AM1_HF</th>\n",
       "      <th>AM1_HOMO</th>\n",
       "      <th>AM1_IP</th>\n",
       "      <th>AM1_LUMO</th>\n",
       "      <th>apol</th>\n",
       "      <th>ASA</th>\n",
       "      <th>ASA+</th>\n",
       "      <th>...</th>\n",
       "      <th>vsurf_Wp4</th>\n",
       "      <th>vsurf_Wp5</th>\n",
       "      <th>vsurf_Wp6</th>\n",
       "      <th>vsurf_Wp7</th>\n",
       "      <th>vsurf_Wp8</th>\n",
       "      <th>Weight</th>\n",
       "      <th>weinerPath</th>\n",
       "      <th>weinerPol</th>\n",
       "      <th>zagreb</th>\n",
       "      <th>Active?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1.136000e+03</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.066978</td>\n",
       "      <td>-113044.050987</td>\n",
       "      <td>-8.826152e+05</td>\n",
       "      <td>106.558253</td>\n",
       "      <td>-10.723729</td>\n",
       "      <td>10.723729</td>\n",
       "      <td>-3.254098</td>\n",
       "      <td>63.784534</td>\n",
       "      <td>684.033082</td>\n",
       "      <td>226.635256</td>\n",
       "      <td>...</td>\n",
       "      <td>27.285307</td>\n",
       "      <td>11.041009</td>\n",
       "      <td>3.150768</td>\n",
       "      <td>0.465351</td>\n",
       "      <td>0.204386</td>\n",
       "      <td>402.862133</td>\n",
       "      <td>2712.382456</td>\n",
       "      <td>44.499123</td>\n",
       "      <td>150.926316</td>\n",
       "      <td>0.877193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.444024</td>\n",
       "      <td>24800.895337</td>\n",
       "      <td>2.471996e+05</td>\n",
       "      <td>98.652237</td>\n",
       "      <td>1.334686</td>\n",
       "      <td>1.334686</td>\n",
       "      <td>1.355129</td>\n",
       "      <td>13.680479</td>\n",
       "      <td>119.536328</td>\n",
       "      <td>53.105243</td>\n",
       "      <td>...</td>\n",
       "      <td>13.156436</td>\n",
       "      <td>5.733696</td>\n",
       "      <td>2.762261</td>\n",
       "      <td>1.618552</td>\n",
       "      <td>0.843494</td>\n",
       "      <td>83.473589</td>\n",
       "      <td>1481.272611</td>\n",
       "      <td>10.137033</td>\n",
       "      <td>32.513735</td>\n",
       "      <td>0.328360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-192978.810000</td>\n",
       "      <td>-1.686050e+06</td>\n",
       "      <td>-284.790990</td>\n",
       "      <td>-13.293860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.643900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.317392</td>\n",
       "      <td>-130268.812500</td>\n",
       "      <td>-1.050144e+06</td>\n",
       "      <td>46.052478</td>\n",
       "      <td>-11.414760</td>\n",
       "      <td>10.411235</td>\n",
       "      <td>-3.836905</td>\n",
       "      <td>54.429473</td>\n",
       "      <td>611.961180</td>\n",
       "      <td>190.567120</td>\n",
       "      <td>...</td>\n",
       "      <td>17.843750</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>363.431000</td>\n",
       "      <td>1666.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.100899</td>\n",
       "      <td>-117269.140000</td>\n",
       "      <td>-9.146876e+05</td>\n",
       "      <td>122.866595</td>\n",
       "      <td>-11.063360</td>\n",
       "      <td>11.063360</td>\n",
       "      <td>-3.695815</td>\n",
       "      <td>66.585411</td>\n",
       "      <td>701.218750</td>\n",
       "      <td>225.864350</td>\n",
       "      <td>...</td>\n",
       "      <td>29.375000</td>\n",
       "      <td>11.687500</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>416.543485</td>\n",
       "      <td>2742.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.404353</td>\n",
       "      <td>-99757.355500</td>\n",
       "      <td>-7.211757e+05</td>\n",
       "      <td>171.432648</td>\n",
       "      <td>-10.411235</td>\n",
       "      <td>11.414760</td>\n",
       "      <td>-3.529475</td>\n",
       "      <td>72.757557</td>\n",
       "      <td>767.038117</td>\n",
       "      <td>264.025365</td>\n",
       "      <td>...</td>\n",
       "      <td>33.875000</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>456.706990</td>\n",
       "      <td>3665.500000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.836674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>469.338440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.293860</td>\n",
       "      <td>5.229690</td>\n",
       "      <td>149.030640</td>\n",
       "      <td>1195.315300</td>\n",
       "      <td>381.133420</td>\n",
       "      <td>...</td>\n",
       "      <td>74.125000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>916.131960</td>\n",
       "      <td>19259.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>362.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 436 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AM1_dipole          AM1_E      AM1_Eele       AM1_HF     AM1_HOMO  \\\n",
       "count  1140.000000    1136.000000  1.136000e+03  1136.000000  1136.000000   \n",
       "mean     12.066978 -113044.050987 -8.826152e+05   106.558253   -10.723729   \n",
       "std       7.444024   24800.895337  2.471996e+05    98.652237     1.334686   \n",
       "min       0.000000 -192978.810000 -1.686050e+06  -284.790990   -13.293860   \n",
       "25%       6.317392 -130268.812500 -1.050144e+06    46.052478   -11.414760   \n",
       "50%      11.100899 -117269.140000 -9.146876e+05   122.866595   -11.063360   \n",
       "75%      16.404353  -99757.355500 -7.211757e+05   171.432648   -10.411235   \n",
       "max      45.836674       0.000000  0.000000e+00   469.338440     0.000000   \n",
       "\n",
       "            AM1_IP     AM1_LUMO         apol          ASA         ASA+  ...  \\\n",
       "count  1136.000000  1136.000000  1140.000000  1140.000000  1140.000000  ...   \n",
       "mean     10.723729    -3.254098    63.784534   684.033082   226.635256  ...   \n",
       "std       1.334686     1.355129    13.680479   119.536328    53.105243  ...   \n",
       "min       0.000000    -7.643900     0.000000     0.000000     0.000000  ...   \n",
       "25%      10.411235    -3.836905    54.429473   611.961180   190.567120  ...   \n",
       "50%      11.063360    -3.695815    66.585411   701.218750   225.864350  ...   \n",
       "75%      11.414760    -3.529475    72.757557   767.038117   264.025365  ...   \n",
       "max      13.293860     5.229690   149.030640  1195.315300   381.133420  ...   \n",
       "\n",
       "         vsurf_Wp4    vsurf_Wp5    vsurf_Wp6    vsurf_Wp7    vsurf_Wp8  \\\n",
       "count  1140.000000  1140.000000  1140.000000  1140.000000  1140.000000   \n",
       "mean     27.285307    11.041009     3.150768     0.465351     0.204386   \n",
       "std      13.156436     5.733696     2.762261     1.618552     0.843494   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      17.843750     7.000000     1.500000     0.000000     0.000000   \n",
       "50%      29.375000    11.687500     2.875000     0.000000     0.000000   \n",
       "75%      33.875000    13.375000     3.625000     0.125000     0.000000   \n",
       "max      74.125000    29.500000    15.250000     8.125000     4.500000   \n",
       "\n",
       "            Weight    weinerPath    weinerPol       zagreb      Active?  \n",
       "count  1140.000000   1140.000000  1140.000000  1140.000000  1140.000000  \n",
       "mean    402.862133   2712.382456    44.499123   150.926316     0.877193  \n",
       "std      83.473589   1481.272611    10.137033    32.513735     0.328360  \n",
       "min       0.000000      0.000000     0.000000     0.000000     0.000000  \n",
       "25%     363.431000   1666.000000    39.000000   132.000000     1.000000  \n",
       "50%     416.543485   2742.500000    45.000000   156.000000     1.000000  \n",
       "75%     456.706990   3665.500000    51.000000   174.000000     1.000000  \n",
       "max     916.131960  19259.000000   126.000000   362.000000     1.000000  \n",
       "\n",
       "[8 rows x 436 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge active and inactive datasets\n",
    "\n",
    "merged_data = pd.concat([actives, inactives], axis = 0)\n",
    "merged_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1000\n",
      "0     140\n",
      "Name: Active?, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# create feature and target columns\n",
    "\n",
    "X = merged_data.drop(columns = ['Active?'])\n",
    "X = X.loc[:, X.apply(pd.Series.nunique) != 1] #remove constant columns\n",
    "y = merged_data['Active?']\n",
    "\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above we have an imbalance of data. This will need to be addressed when scoring different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Predictive Models with Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components needed for 95% explained variance:  54\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with 435 features, we need to narrow down the range using PCA\n",
    "\n",
    "pca_plot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need about 54 components to explain 95 % variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline for imputing, scaling and PCA/feature selection. Also selecting sampler for imbalance in dataset.\n",
    "\n",
    "numerical_transformer = Pipeline(steps = [('impute', SimpleImputer(strategy = 'mean')),\n",
    "                                          ('scaler', preprocessing.StandardScaler()), \n",
    "                                          ('selector', PCA(n_components = 0.95))]) \n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('num', numerical_transformer, list(X.columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "jaccard index: [0.70952381 0.71770335 0.72815534 0.79069767 0.76699029] 0.7426140928585345\n",
      "f1-value: [0.60060879 0.61370359 0.64134831 0.62465703 0.67319637] 0.6307028192624076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "jaccard index: [0.77073171 0.705      0.77669903 0.83251232 0.84      ] 0.7849886103428445\n",
      "f1-value: [0.68257354 0.656968   0.68160291 0.75192012 0.77470356] 0.7095536254951668\n",
      "KNN\n",
      "jaccard index: [0.87264151 0.84541063 0.78672986 0.82629108 0.82524272] 0.8312631587063999\n",
      "f1-value: [0.7371814  0.74189897 0.65550818 0.67632276 0.72712766] 0.7076077923403729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "jaccard index: [0.86602871 0.69117647 0.75980392 0.80582524 0.80097087] 0.7847610433591377\n",
      "f1-value: [0.75198135 0.62491187 0.67917756 0.70814132 0.70356746] 0.6935559122403896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "jaccard index: [0.62801932 0.67464115 0.6195122  0.67307692 0.6127451 ] 0.6415989376469893\n",
      "f1-value: [0.56222726 0.58210243 0.56801399 0.58748404 0.56891559] 0.5737486611115871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#run predictive models with default settings for random undersampling\n",
    "\n",
    "model_cross_validation_for_select_params(X, y, 5, preprocessor = preprocessor, sampler = RandomUnderSampler(random_state = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "jaccard index: [0.90366972 0.90740741 0.89351852 0.86036036 0.90232558] 0.8934563184904555\n",
      "f1-value: [0.71860123 0.74845543 0.72720179 0.60200462 0.75092337] 0.7094372884429946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "jaccard index: [0.88105727 0.88053097 0.87280702 0.87665198 0.88888889] 0.8799872261970794\n",
      "f1-value: [0.50286683 0.53275142 0.46604215 0.50046948 0.56736243] 0.5138984647378744\n",
      "KNN\n",
      "jaccard index: [0.90186916 0.90291262 0.85981308 0.86697248 0.9223301 ] 0.8907794877002952\n",
      "f1-value: [0.75991576 0.8182398  0.70369087 0.6684551  0.84646465] 0.7593532334918143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "jaccard index: [0.8957346  0.76097561 0.80193237 0.81904762 0.79326923] 0.8141918847758207\n",
      "f1-value: [0.77607143 0.67423823 0.69805226 0.69350502 0.6833231 ] 0.7050380092975252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "jaccard index: [0.88392857 0.875      0.87614679 0.85526316 0.89545455] 0.8771586127537357\n",
      "f1-value: [0.58684137 0.57777778 0.67975862 0.46099291 0.67755027] 0.5965841894321484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#run predictive models with default settings for random oversampling\n",
    "\n",
    "model_cross_validation_for_select_params(X, y, 5, preprocessor = preprocessor, sampler = RandomOverSampler(random_state = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "jaccard index: [0.88990826 0.86511628 0.87264151 0.83555556 0.86511628] 0.8656675760019574\n",
      "f1-value: [0.69814651 0.70020404 0.7371814  0.52497325 0.70020404] 0.672141848061165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "jaccard index: [0.88444444 0.88789238 0.88053097 0.8722467  0.88392857] 0.88180861240824\n",
      "f1-value: [0.56308962 0.61316593 0.53275142 0.49814042 0.58684137] 0.558797753264835\n",
      "KNN\n",
      "jaccard index: [0.83253589 0.90686275 0.87735849 0.86511628 0.90243902] 0.8768624848583105\n",
      "f1-value: [0.71458207 0.83378736 0.74319875 0.70020404 0.82284382] 0.762923207716724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "jaccard index: [0.90610329 0.76097561 0.78947368 0.83018868 0.8125    ] 0.8198482519193767\n",
      "f1-value: [0.77536946 0.67423823 0.67288379 0.68890237 0.70144042] 0.7025668514074059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "jaccard index: [0.86486486 0.85648148 0.85116279 0.87962963 0.83962264] 0.8583522816366169\n",
      "f1-value: [0.60662526 0.67952845 0.68393693 0.7079803  0.6988345 ] 0.6753810861615615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#run predictive models with default settings for SMOTE\n",
    "\n",
    "model_cross_validation_for_select_params(X, y, 5, preprocessor = preprocessor, sampler = SMOTE(random_state = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So it seems that with default parameters, undersampling performs worse than oversampling, with both random oversampling and SMOTE performing similarly. We will therefore continue with random oversampling purely due to lower computation cost. XGBoost, KNN, and Random Forest perform similarly and better than the remaining two (SVM and LR) with default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameters using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "{'model__learning_rate': 0.2, 'model__n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "{'model__C': 10, 'model__gamma': 0.001}\n",
      "KNN\n",
      "{'model__n_neighbors': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\n",
      "{'model__C': 1, 'model__penalty': 'l1', 'model__solver': 'liblinear'}\n",
      "RandomForest\n",
      "{'model__max_features': 3, 'model__n_estimators': 750}\n"
     ]
    }
   ],
   "source": [
    "# note parameters are optimised according to f1 macro score\n",
    "\n",
    "model_grid_search(X, y, 5, preprocessor = preprocessor, sampler = RandomUnderSampler(random_state = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "jaccard index: [0.8959276  0.89041096 0.88073394 0.86222222 0.92056075] 0.8899710951107933\n",
      "f1-value: [0.66174289 0.68530021 0.68568399 0.54408824 0.79042985] 0.6734490339354211\n",
      "SVM\n",
      "jaccard index: [0.9255814  0.91509434 0.92417062 0.85650224 0.90821256] 0.9059122307248326\n",
      "f1-value: [0.79020014 0.79783251 0.82029557 0.58040028 0.82021166] 0.7617880310133968\n",
      "KNN\n",
      "jaccard index: [0.88687783 0.91037736 0.89449541 0.87837838 0.92857143] 0.8997400812677417\n",
      "f1-value: [0.64951116 0.7902687  0.7047131  0.62147205 0.83442266] 0.7200775349338439\n",
      "LR\n",
      "jaccard index: [0.8957346  0.76585366 0.79710145 0.81904762 0.80769231] 0.8170859263416543\n",
      "f1-value: [0.77607143 0.67838251 0.69354839 0.69350502 0.69680851] 0.7076631716353698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      "jaccard index: [0.88495575 0.88495575 0.87280702 0.87719298 0.88495575] 0.8809734513274335\n",
      "f1-value: [0.53615023 0.53615023 0.46604215 0.46728972 0.53615023] 0.5083565156836529\n"
     ]
    }
   ],
   "source": [
    "#test models with improved parameters\n",
    "\n",
    "model_cross_validation_for_select_params(X, y, 5, preprocessor = preprocessor, sampler = RandomOverSampler(random_state = 0),\n",
    "                                         models = [XGBClassifier(random_state = 0, learning_rate = 0.2, n_estimators = 1000), \n",
    "                                         svm.SVC(random_state = 0, C = 10, gamma = 0.001), \n",
    "                                         KNeighborsClassifier(n_neighbors = 1), \n",
    "                                         LogisticRegression(random_state = 0, C = 1, penalty = 'l1', solver = 'liblinear'), \n",
    "                                         RandomForestClassifier(random_state = 0, max_features = 3, n_estimators = 750)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best performance from SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and target columns\n",
    "\n",
    "X = merged_data.drop(columns = ['Active?'])\n",
    "X = X.loc[:, X.apply(pd.Series.nunique) != 1] #remove constant columns\n",
    "y = merged_data['Active?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with NaNs, normalise, and reduce dimensionality\n",
    "\n",
    "numerical_transformer_nn = Pipeline(steps = [('impute', SimpleImputer(strategy = 'mean')),\n",
    "                                          ('scaler', preprocessing.StandardScaler()),\n",
    "                                          ('selector', PCA(n_components = 0.95))]) \n",
    "\n",
    "preprocessor_nn = ColumnTransformer(transformers = [('num', numerical_transformer_nn, list(X.columns))])\n",
    "\n",
    "pipeline_nn = Pipeline(steps = [('preprocessor', preprocessor_nn)])\n",
    "\n",
    "X = pipeline_nn.fit_transform(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test sets, note 'stratify' is automatically carried out in cross_validate and GridSearch above,\n",
    "# but not for train_test_split. Stratify ensures that the dataset is split proportionally taking into account class\n",
    "# labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400,) (342,)\n"
     ]
    }
   ],
   "source": [
    "# oversample X_train to address data imbalance. Needs to be done after splitting data!\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)  \n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following is needed to run with the neural network code above (change from y.shape = (X,) to (1, X))\n",
    "\n",
    "y_train = y_train.reshape((1, y_train.shape[0]))\n",
    "y_test = np.array(y_test).reshape((1, y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 55 input features, 3 layer network, number of nodes for 1st and 2nd later have not been optimised, neither\n",
    "# has number of layers.\n",
    "\n",
    "layers_dims = [55, 5, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693149\n",
      "Cost after iteration 100: 0.693130\n",
      "Cost after iteration 200: 0.693075\n",
      "Cost after iteration 300: 0.692692\n",
      "Cost after iteration 400: 0.654813\n",
      "Cost after iteration 500: 0.448456\n",
      "Cost after iteration 600: 0.385061\n",
      "Cost after iteration 700: 0.341397\n",
      "Cost after iteration 800: 0.267873\n",
      "Cost after iteration 900: 0.225488\n",
      "Cost after iteration 1000: 0.193014\n",
      "Cost after iteration 1100: 0.162890\n",
      "Cost after iteration 1200: 0.102272\n",
      "Cost after iteration 1300: 0.082104\n",
      "Cost after iteration 1400: 0.067793\n",
      "Cost after iteration 1500: 0.068163\n",
      "Cost after iteration 1600: 0.036527\n",
      "Cost after iteration 1700: 0.033075\n",
      "Cost after iteration 1800: 0.029193\n",
      "Cost after iteration 1900: 0.026516\n",
      "Cost after iteration 2000: 0.026415\n",
      "Cost after iteration 2100: 0.022994\n",
      "Cost after iteration 2200: 0.020898\n",
      "Cost after iteration 2300: 0.019567\n",
      "Cost after iteration 2400: 0.018501\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train.T, y_train, layers_dims, learning_rate = 0.075, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard index: 0.9845288326300985\n",
      "f1-value: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(X_train.T, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard index: 0.9213836477987422\n",
      "f1-value: 0.8567251461988304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  'and multiclass classification tasks.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(X_test.T, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In terms of f1-value, the neural network, although not explicitely optimised, appears to be capable of performing better than SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
